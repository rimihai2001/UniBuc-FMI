{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Laborator4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Exercitiul 2\n",
        "\n",
        "Pentru acest exercitui am implementat 2 variante ale metodei pentru normalizare.\n",
        "\n",
        "Prima metoda foloseste libraria numpy si a 2-a foloseste scikit-learn.\n",
        "\n"
      ],
      "metadata": {
        "id": "Z6onTYgkmmix"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "R4PfWgQHOAQO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "def normalize_data(train_data, test_data, type=None):\n",
        "  if type =='standard':\n",
        "    mean = np.mean(train_data,axis=0) # calculeaza mediile atributelor din datele de antrenare\n",
        "    std = np.std(train_data,axis=0) # calculeaza deviatiile standard din datele de antrenare\n",
        "    train_data -= mean\n",
        "    train_data/=std\n",
        "    test_data -= mean\n",
        "    test_data /=std\n",
        "  elif type == 'l1':\n",
        "    train_data = train_data / (np.expand_dims((np.linalg.norm(train_data,ord=1,axis=1)), axis=1) + 1e-6) # normalizarea L1. 1e-6 este adunat pt cazurile in care norma e 0. Iar np.expand_dims e folosit pentru a putea folosi broadcast la impartire\n",
        "    test_data = test_data / (np.expand_dims((np.linalg.norm(test_data, ord=1,axis=1)), axis=1) + 1e-6)\n",
        "  elif type == 'l2':\n",
        "    norm_train = np.expand_dims((np.linalg.norm(train_data,ord=2,axis=1)), axis=1)\n",
        "    train_data = train_data / (norm_train + 1e-6)\n",
        "    test_data = test_data / (np.expand_dims((np.linalg.norm(test_data, ord=2,axis=1)), axis=1) +1e-6)\n",
        "  return train_data, test_data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler, Normalizer\n",
        "def normalize_data_v2(train_data, test_data, type=None):\n",
        "  if type=='standard':\n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit(train_data)\n",
        "    train_data = scaler.transform(train_data)\n",
        "    test_data = scaler.transform(test_data)\n",
        "  elif type=='l2':\n",
        "    normalizer = Normalizer(norm='l2')\n",
        "    train_data = normalizer.transform(train_data)\n",
        "    test_data = normalizer.transform(test_data)\n",
        "  elif type =='l1':\n",
        "    normalizer = Normalizer(norm='l1')\n",
        "    train_data = normalizer.transform(train_data)\n",
        "    test_data = normalizer.transform(test_data)\n",
        "  return train_data, test_data"
      ],
      "metadata": {
        "id": "F_OoiyXwPvDN"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercitiul 3 si 4"
      ],
      "metadata": {
        "id": "RfUzsMfboh29"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "class BagOfWords:\n",
        "\n",
        "  def __init__(self):\n",
        "    self.vocab = {}\n",
        "    self.words = []\n",
        "\n",
        "  def build_vocabulary(self, data):\n",
        "    # pentru fiecare cuvant din variabila \"data\" asociem un id unic,\n",
        "    # care va reprezenta indexul cuvantului in vectorul de features\n",
        "    id_word = 0\n",
        "    for mesaj in data:\n",
        "      for word in mesaj:\n",
        "        if word not in self.vocab:\n",
        "          self.vocab[word]=id_word\n",
        "          id_word+=1\n",
        "          self.words.append(word)\n",
        "  \n",
        "  def get_features(self, data):\n",
        "    # transforma textele din var \"data\" in vectori de dimensiunea vocabularului\n",
        "    # care contin frecventele cuvintelor.\n",
        "    feature_matrix = np.zeros((data.shape[0],len(self.vocab)))\n",
        "    for i, example in enumerate(data):\n",
        "      no_word = True\n",
        "      for word in example:\n",
        "        if word in self.vocab:\n",
        "          feature_matrix[i,self.vocab[word]] +=1\n",
        "          no_word = False\n",
        "      if no_word:\n",
        "        print(example)\n",
        "    return feature_matrix"
      ],
      "metadata": {
        "id": "w3Si9dIERVFq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_original = np.load('./data/training_sentences.npy',allow_pickle=True)\n",
        "train_labels = np.load('./data/training_labels.npy')\n",
        "test_data_original = np.load('./data/test_sentences.npy',allow_pickle=True)\n",
        "test_labels = np.load('./data/test_labels.npy')"
      ],
      "metadata": {
        "id": "hmm8hy-6R9Ds"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bow = BagOfWords()\n",
        "bow.build_vocabulary(train_data_original)\n",
        "print(len(bow.vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xjk4NBFUhLN",
        "outputId": "18e0eadb-e3a1-4970-95f2-5839f44816c7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9522\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "RF601eKMZTfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercitiul 5\n",
        "\n",
        "Urmatoarele 2 celule reprezinta ex5."
      ],
      "metadata": {
        "id": "u1BAJoWRpkrd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_original = bow.get_features(train_data_original)\n",
        "test_data_original = bow.get_features(test_data_original)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ki4jnV_eUPtG",
        "outputId": "28685ce1-b613-4a6e-bda7-6247bd6bb409"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n",
            "[]\n",
            "['Ultimately', 'tor', 'motive', 'tui', 'achieve', 'korli']\n",
            "['22', '146tf150p']\n",
            "['YO', 'YO', 'YO', 'BYATCH', 'WHASSUP']\n",
            "['Ryder', 'unsoldnow', 'gibbs']\n",
            "['Kkcongratulation']\n",
            "['hanks', 'lotsly']\n",
            "['645']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, test_data = normalize_data_v2(train_data_original,test_data_original,type='l2')"
      ],
      "metadata": {
        "id": "a8GKkwdlU0_N"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercitiul 6\n",
        "\n",
        "Pentru antrenarea modelului folosim functia fit.\n",
        "\n",
        "Iar cu functia predict obtinem predictiile pentru setul de test."
      ],
      "metadata": {
        "id": "ivFq2KA_pu1g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import svm\n",
        "svm_model = svm.SVC(C=1,kernel= \"linear\") # define the model\n",
        "svm_model.fit(train_data, train_labels) # train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fApYd7IHUZcY",
        "outputId": "fafc8a42-3b07-485d-d42c-5a9a59981e02"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(C=1, kernel='linear')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "\n",
        "pred_labels = svm_model.predict(test_data)\n",
        "print(\"Accuracy:\", accuracy_score(test_labels,pred_labels),\"F1-score:\", f1_score(test_labels, pred_labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ozJpCY3_U-ml",
        "outputId": "cc53628b-0ad6-4679-b12f-fbefb4b7c9a6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9842391304347826 F1-score 0.9409368635437881\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mesajele spam au label-ul 1, iar cele non-spam au label-ul 0. \n",
        "\n",
        "Deci cele mai negative cuvinte sunt cele care in functia de decizie au cel mai mare coeficient(>0 bineinteles), iar cele mai pozitive sunt cele cu coeficient mic (<0)"
      ],
      "metadata": {
        "id": "tkwQhB3UqfXH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index_sort = np.argsort(svm_model.coef_[0])\n",
        "bow.words = np.array(bow.words)\n",
        "print(\"Most positive words:\", bow.words[index_sort[:10]])\n",
        "print(\"Most negative words:\", bow.words[index_sort[-10:]])\n",
        "\n"
      ],
      "metadata": {
        "id": "71tAUeS1rDCR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75164e24-203f-4df2-f8b5-ba1d9c70cd21"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most positive words: ['&lt#&gt' 'me' 'i' 'Going' 'him' 'Ok' 'I' 'Ill' 'my' 'Im']\n",
            "Most negative words: ['Text' 'To' 'mobile' 'CALL' 'FREE' 'txt' '&' 'Call' 'Txt' 'STOP']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "dQpw8rpPSttp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip ./data_lab5.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLlINPb3PrMj",
        "outputId": "2c2200e0-b0cf-467d-a3db-935bded36b5f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  ./data_lab5.zip\n",
            "  inflating: data/test_labels.npy    \n",
            "  inflating: data/test_sentences.npy  \n",
            "  inflating: data/training_labels.npy  \n",
            "  inflating: data/training_sentences.npy  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "mF8XK9OARUOt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}